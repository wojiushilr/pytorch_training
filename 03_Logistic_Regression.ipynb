{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Logistic_Regression.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMnZ2xeKNhXYDWll7SLPm/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wojiushilr/pytorch_training/blob/master/03_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kp0d9IZljq2",
        "colab_type": "text"
      },
      "source": [
        "### 引包定义超参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tCgJj1RaWz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b4bChfRivrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 定义超参数\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 100"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYDIng30lpy0",
        "colab_type": "text"
      },
      "source": [
        "### 下载数据集mnist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQKVzuVglv9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = datasets.FashionMNIST(\n",
        "    root='../datasets', train=True, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root='../datasets', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Vl8N9Bv9fu",
        "colab_type": "text"
      },
      "source": [
        "### 看看数据集里都是什么格式\n",
        "### 看起来data[0] -> image data\n",
        "### data[1] -> imgae label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzwehPmlmmIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(type(train_dataset))\n",
        "print(train_loader)\n",
        "for i, data in enumerate(train_loader,0):\n",
        "  print(data[1])\n",
        "  print(type(data)) ## <class 'list'>\n",
        "  print(type(data[1])) ## <class 'torch.Tensor'>\n",
        "  print(len(data[1]))\n",
        "  print(i)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elQl2DtVpSkt",
        "colab_type": "text"
      },
      "source": [
        "### 定义模型， 如果有GPU则使用GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEsof8_Do3Yz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# logistic Regression Model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self,in_dim, n_class):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.logistic = nn.Linear(in_dim, n_class)  # input and output is 1 dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.logistic(x)\n",
        "        return out\n",
        "  \n",
        "\n",
        "model = LogisticRegression(28*28 ,10)\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-GQd2YHtk3R",
        "colab_type": "text"
      },
      "source": [
        "### 训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSozr_YVtkRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eaaec56d-81e4-4c24-b2cb-1bc2d9b254f7"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  starttime = time.time()\n",
        "  train_acc = 0.0\n",
        "  train_loss = 0.0\n",
        "  val_acc = 0.0\n",
        "  val_loss = 0.0\n",
        "  for i, data in enumerate(train_loader):\n",
        "    img, label = data\n",
        "    img = img.view(img.size(0), -1)  # 将图片展开成 28x28\n",
        "    if torch.cuda.is_available():\n",
        "      img = img.cuda()\n",
        "      label = label.cuda()\n",
        "    # 向前传播\n",
        "    train_pred = model(img)\n",
        "    batch_loss = loss(train_pred, label)\n",
        "    train_loss += batch_loss.item()\n",
        "    _, pred = torch.max(train_pred, 1)\n",
        "    train_acc += (pred==label).float().mean()\n",
        "    optimizer.zero_grad()\n",
        "    batch_loss.backward() # 算出每個參數的 gradient\n",
        "    optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
        "  if i % 300 == 0:\n",
        "    # print(f'[{epoch+1}/{num_epochs}] Loss: {train_loss/i:.6f}, Acc: {train_acc　/i:.6f}')\n",
        "    print(\"{} / {} Loss: {}, Acc: {}\".format(epoch+1,num_epochs, train_loss/i, train_acc/i))\n",
        "  print(\"{} / {} Loss: {}, Acc: {}\".format(epoch+1, num_epochs, train_loss/i, train_acc/i))\n",
        "    \n",
        "  model.eval()\n",
        "  for data in test_loader:\n",
        "      img, label = data\n",
        "      img = img.view(img.size(0), -1)\n",
        "      if torch.cuda.is_available():\n",
        "          img = img.cuda()\n",
        "          label = label.cuda()\n",
        "      with torch.no_grad():\n",
        "          val_pred = model(img)\n",
        "          batch_loss = loss(val_pred, label)\n",
        "      val_loss += batch_loss.item()\n",
        "      _, pred = torch.max(val_pred, 1)\n",
        "      val_acc += (pred == label).float().mean()\n",
        "  print(f'Test Loss: {val_loss/len(test_loader):.6f}, Acc: {val_acc/len(test_loader):.6f}')\n",
        "  print(f'Time:{(time.time()-starttime):.1f} s')\n",
        "\n",
        "# 保存模型\n",
        "torch.save(model.state_dict(), './logstic.pth')\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 100 Loss: 1.1942947496090526, Acc: 0.67445969581604\n",
            "Test Loss: 1.093525, Acc: 0.675159\n",
            "Time:5.2 s\n",
            "2 / 100 Loss: 1.0174725434187255, Acc: 0.6949872970581055\n",
            "Test Loss: 0.975108, Acc: 0.692775\n",
            "Time:5.2 s\n",
            "3 / 100 Loss: 0.9251946252845268, Acc: 0.716582179069519\n",
            "Test Loss: 0.904646, Acc: 0.708897\n",
            "Time:5.4 s\n",
            "4 / 100 Loss: 0.8661730503196269, Acc: 0.7335412502288818\n",
            "Test Loss: 0.856675, Acc: 0.721238\n",
            "Time:5.2 s\n",
            "5 / 100 Loss: 0.8243360663172784, Acc: 0.7455142736434937\n",
            "Test Loss: 0.821812, Acc: 0.735569\n",
            "Time:5.2 s\n",
            "6 / 100 Loss: 0.7922676682726678, Acc: 0.7549692988395691\n",
            "Test Loss: 0.793247, Acc: 0.739351\n",
            "Time:5.3 s\n",
            "7 / 100 Loss: 0.7666660552345447, Acc: 0.7621397972106934\n",
            "Test Loss: 0.770790, Acc: 0.748209\n",
            "Time:5.2 s\n",
            "8 / 100 Loss: 0.745429297330667, Acc: 0.7690101265907288\n",
            "Test Loss: 0.751691, Acc: 0.754080\n",
            "Time:5.1 s\n",
            "9 / 100 Loss: 0.7274475555664193, Acc: 0.7743462920188904\n",
            "Test Loss: 0.735334, Acc: 0.759554\n",
            "Time:5.3 s\n",
            "10 / 100 Loss: 0.7120665878916245, Acc: 0.7787320017814636\n",
            "Test Loss: 0.721279, Acc: 0.764630\n",
            "Time:5.6 s\n",
            "11 / 100 Loss: 0.698488668354751, Acc: 0.7826007008552551\n",
            "Test Loss: 0.708900, Acc: 0.768909\n",
            "Time:5.1 s\n",
            "12 / 100 Loss: 0.6863943878143803, Acc: 0.7867862582206726\n",
            "Test Loss: 0.697971, Acc: 0.772890\n",
            "Time:5.4 s\n",
            "13 / 100 Loss: 0.675718268532636, Acc: 0.7899379730224609\n",
            "Test Loss: 0.688180, Acc: 0.775378\n",
            "Time:5.3 s\n",
            "14 / 100 Loss: 0.6659381303741431, Acc: 0.7932897806167603\n",
            "Test Loss: 0.679065, Acc: 0.778364\n",
            "Time:5.1 s\n",
            "15 / 100 Loss: 0.6572721791369175, Acc: 0.7953408360481262\n",
            "Test Loss: 0.671007, Acc: 0.781350\n",
            "Time:5.2 s\n",
            "16 / 100 Loss: 0.649204500965782, Acc: 0.7973752617835999\n",
            "Test Loss: 0.663575, Acc: 0.783041\n",
            "Time:5.2 s\n",
            "17 / 100 Loss: 0.6417612521218389, Acc: 0.7998432517051697\n",
            "Test Loss: 0.657092, Acc: 0.784634\n",
            "Time:5.3 s\n",
            "18 / 100 Loss: 0.6350266449988333, Acc: 0.8012606501579285\n",
            "Test Loss: 0.650767, Acc: 0.786425\n",
            "Time:5.2 s\n",
            "19 / 100 Loss: 0.6285495535731952, Acc: 0.803195059299469\n",
            "Test Loss: 0.644477, Acc: 0.789610\n",
            "Time:5.5 s\n",
            "20 / 100 Loss: 0.6227567397543053, Acc: 0.8049960136413574\n",
            "Test Loss: 0.639452, Acc: 0.789908\n",
            "Time:5.3 s\n",
            "21 / 100 Loss: 0.617232032303113, Acc: 0.8065468072891235\n",
            "Test Loss: 0.634065, Acc: 0.790605\n",
            "Time:5.2 s\n",
            "22 / 100 Loss: 0.6121184201543334, Acc: 0.8070971369743347\n",
            "Test Loss: 0.629270, Acc: 0.792396\n",
            "Time:5.1 s\n",
            "23 / 100 Loss: 0.6073385079807762, Acc: 0.808464527130127\n",
            "Test Loss: 0.624630, Acc: 0.794088\n",
            "Time:5.1 s\n",
            "24 / 100 Loss: 0.6026660025183393, Acc: 0.8100653886795044\n",
            "Test Loss: 0.620518, Acc: 0.795482\n",
            "Time:5.4 s\n",
            "25 / 100 Loss: 0.5982693435351902, Acc: 0.8108824491500854\n",
            "Test Loss: 0.616517, Acc: 0.796676\n",
            "Time:5.5 s\n",
            "26 / 100 Loss: 0.5940809574300222, Acc: 0.8119330406188965\n",
            "Test Loss: 0.612684, Acc: 0.799463\n",
            "Time:5.3 s\n",
            "27 / 100 Loss: 0.5904935911663568, Acc: 0.812816858291626\n",
            "Test Loss: 0.609022, Acc: 0.800259\n",
            "Time:5.2 s\n",
            "28 / 100 Loss: 0.5864873247632604, Acc: 0.8133004307746887\n",
            "Test Loss: 0.605995, Acc: 0.799463\n",
            "Time:5.2 s\n",
            "29 / 100 Loss: 0.5832819177857586, Acc: 0.8147345185279846\n",
            "Test Loss: 0.602361, Acc: 0.802050\n",
            "Time:5.3 s\n",
            "30 / 100 Loss: 0.5797168413373933, Acc: 0.8154849410057068\n",
            "Test Loss: 0.599561, Acc: 0.802349\n",
            "Time:5.3 s\n",
            "31 / 100 Loss: 0.5765096530270551, Acc: 0.8164854645729065\n",
            "Test Loss: 0.596182, Acc: 0.805135\n",
            "Time:5.4 s\n",
            "32 / 100 Loss: 0.5734570039120148, Acc: 0.8169857263565063\n",
            "Test Loss: 0.593770, Acc: 0.804737\n",
            "Time:5.2 s\n",
            "33 / 100 Loss: 0.5704763356433226, Acc: 0.8179028630256653\n",
            "Test Loss: 0.590736, Acc: 0.805932\n",
            "Time:5.4 s\n",
            "34 / 100 Loss: 0.5678013820467535, Acc: 0.8188533782958984\n",
            "Test Loss: 0.588227, Acc: 0.807026\n",
            "Time:5.2 s\n",
            "35 / 100 Loss: 0.5650976047699072, Acc: 0.8196038007736206\n",
            "Test Loss: 0.585647, Acc: 0.808420\n",
            "Time:5.2 s\n",
            "36 / 100 Loss: 0.5624749514183851, Acc: 0.8200206756591797\n",
            "Test Loss: 0.583296, Acc: 0.808718\n",
            "Time:5.2 s\n",
            "37 / 100 Loss: 0.5599328249915559, Acc: 0.8207877278327942\n",
            "Test Loss: 0.581050, Acc: 0.808917\n",
            "Time:5.2 s\n",
            "38 / 100 Loss: 0.5576017604248247, Acc: 0.8214214444160461\n",
            "Test Loss: 0.578874, Acc: 0.808917\n",
            "Time:5.5 s\n",
            "39 / 100 Loss: 0.5552317645468351, Acc: 0.8217382431030273\n",
            "Test Loss: 0.576620, Acc: 0.810211\n",
            "Time:5.2 s\n",
            "40 / 100 Loss: 0.5529960624181856, Acc: 0.822438657283783\n",
            "Test Loss: 0.574569, Acc: 0.809813\n",
            "Time:5.1 s\n",
            "41 / 100 Loss: 0.5508825518787225, Acc: 0.8230389356613159\n",
            "Test Loss: 0.572649, Acc: 0.811206\n",
            "Time:5.2 s\n",
            "42 / 100 Loss: 0.5487987025379498, Acc: 0.8239060640335083\n",
            "Test Loss: 0.570721, Acc: 0.811803\n",
            "Time:5.4 s\n",
            "43 / 100 Loss: 0.5467388464839634, Acc: 0.8246731758117676\n",
            "Test Loss: 0.568859, Acc: 0.812201\n",
            "Time:5.2 s\n",
            "44 / 100 Loss: 0.5447881733531504, Acc: 0.8249566555023193\n",
            "Test Loss: 0.567001, Acc: 0.813396\n",
            "Time:5.2 s\n",
            "45 / 100 Loss: 0.5429115015516414, Acc: 0.825957179069519\n",
            "Test Loss: 0.565284, Acc: 0.813197\n",
            "Time:5.4 s\n",
            "46 / 100 Loss: 0.5410495716581477, Acc: 0.8260738849639893\n",
            "Test Loss: 0.563939, Acc: 0.812600\n",
            "Time:5.3 s\n",
            "47 / 100 Loss: 0.5394088334245896, Acc: 0.8265575170516968\n",
            "Test Loss: 0.561937, Acc: 0.813595\n",
            "Time:5.3 s\n",
            "48 / 100 Loss: 0.5375659211022876, Acc: 0.8272912502288818\n",
            "Test Loss: 0.560367, Acc: 0.813495\n",
            "Time:5.5 s\n",
            "49 / 100 Loss: 0.5358386317083141, Acc: 0.8272411823272705\n",
            "Test Loss: 0.558953, Acc: 0.813993\n",
            "Time:5.2 s\n",
            "50 / 100 Loss: 0.5343726720728513, Acc: 0.8275913596153259\n",
            "Test Loss: 0.557352, Acc: 0.814789\n",
            "Time:5.3 s\n",
            "51 / 100 Loss: 0.5326427801759673, Acc: 0.827724814414978\n",
            "Test Loss: 0.555939, Acc: 0.814988\n",
            "Time:5.4 s\n",
            "52 / 100 Loss: 0.5311436360839082, Acc: 0.828441858291626\n",
            "Test Loss: 0.554825, Acc: 0.815187\n",
            "Time:5.2 s\n",
            "53 / 100 Loss: 0.529621129292943, Acc: 0.8288087248802185\n",
            "Test Loss: 0.553129, Acc: 0.815784\n",
            "Time:5.2 s\n",
            "54 / 100 Loss: 0.5281053388703977, Acc: 0.8288920521736145\n",
            "Test Loss: 0.551779, Acc: 0.816083\n",
            "Time:5.3 s\n",
            "55 / 100 Loss: 0.5267273987242862, Acc: 0.8294757008552551\n",
            "Test Loss: 0.550518, Acc: 0.816182\n",
            "Time:5.2 s\n",
            "56 / 100 Loss: 0.5253598615924729, Acc: 0.8291589021682739\n",
            "Test Loss: 0.549312, Acc: 0.816083\n",
            "Time:5.2 s\n",
            "57 / 100 Loss: 0.5242475734003834, Acc: 0.8294256925582886\n",
            "Test Loss: 0.548084, Acc: 0.816680\n",
            "Time:5.2 s\n",
            "58 / 100 Loss: 0.5226544964275339, Acc: 0.829892635345459\n",
            "Test Loss: 0.547078, Acc: 0.816779\n",
            "Time:5.3 s\n",
            "59 / 100 Loss: 0.5214320475607316, Acc: 0.8299926519393921\n",
            "Test Loss: 0.545660, Acc: 0.816083\n",
            "Time:5.3 s\n",
            "60 / 100 Loss: 0.5200487828496426, Acc: 0.8304762244224548\n",
            "Test Loss: 0.544671, Acc: 0.817675\n",
            "Time:5.2 s\n",
            "61 / 100 Loss: 0.5189898444087172, Acc: 0.8304762244224548\n",
            "Test Loss: 0.543421, Acc: 0.818471\n",
            "Time:5.3 s\n",
            "62 / 100 Loss: 0.5176363966826314, Acc: 0.8305429816246033\n",
            "Test Loss: 0.542357, Acc: 0.818372\n",
            "Time:5.2 s\n",
            "63 / 100 Loss: 0.5165711168990954, Acc: 0.8311933279037476\n",
            "Test Loss: 0.541285, Acc: 0.818770\n",
            "Time:5.2 s\n",
            "64 / 100 Loss: 0.5154403133224398, Acc: 0.8312433362007141\n",
            "Test Loss: 0.540406, Acc: 0.818770\n",
            "Time:5.2 s\n",
            "65 / 100 Loss: 0.5142235083857366, Acc: 0.8316435217857361\n",
            "Test Loss: 0.539268, Acc: 0.819666\n",
            "Time:5.2 s\n",
            "66 / 100 Loss: 0.5133213395751782, Acc: 0.8321938514709473\n",
            "Test Loss: 0.538266, Acc: 0.819168\n",
            "Time:5.2 s\n",
            "67 / 100 Loss: 0.5120737041200619, Acc: 0.8323272466659546\n",
            "Test Loss: 0.537407, Acc: 0.819467\n",
            "Time:5.2 s\n",
            "68 / 100 Loss: 0.5110573733788035, Acc: 0.8328108191490173\n",
            "Test Loss: 0.536463, Acc: 0.820462\n",
            "Time:5.2 s\n",
            "69 / 100 Loss: 0.5100504493732463, Acc: 0.8332944512367249\n",
            "Test Loss: 0.535315, Acc: 0.819765\n",
            "Time:5.4 s\n",
            "70 / 100 Loss: 0.5090665707211611, Acc: 0.8331276774406433\n",
            "Test Loss: 0.534476, Acc: 0.820362\n",
            "Time:5.2 s\n",
            "71 / 100 Loss: 0.5081768989753825, Acc: 0.8332610726356506\n",
            "Test Loss: 0.533491, Acc: 0.820561\n",
            "Time:5.3 s\n",
            "72 / 100 Loss: 0.507079294358908, Acc: 0.8332610726356506\n",
            "Test Loss: 0.532702, Acc: 0.820462\n",
            "Time:5.3 s\n",
            "73 / 100 Loss: 0.5062104704985624, Acc: 0.8340448141098022\n",
            "Test Loss: 0.531710, Acc: 0.821059\n",
            "Time:5.2 s\n",
            "74 / 100 Loss: 0.5051885530941005, Acc: 0.8342449069023132\n",
            "Test Loss: 0.531064, Acc: 0.821357\n",
            "Time:5.3 s\n",
            "75 / 100 Loss: 0.5042592190054972, Acc: 0.8344783782958984\n",
            "Test Loss: 0.530227, Acc: 0.821158\n",
            "Time:5.4 s\n",
            "76 / 100 Loss: 0.5034201749806216, Acc: 0.8346784710884094\n",
            "Test Loss: 0.529348, Acc: 0.821756\n",
            "Time:5.6 s\n",
            "77 / 100 Loss: 0.5025575563645541, Acc: 0.8346951603889465\n",
            "Test Loss: 0.528419, Acc: 0.821955\n",
            "Time:5.3 s\n",
            "78 / 100 Loss: 0.5015870509559851, Acc: 0.8350787162780762\n",
            "Test Loss: 0.527795, Acc: 0.822054\n",
            "Time:5.2 s\n",
            "79 / 100 Loss: 0.5007135678826172, Acc: 0.8352954983711243\n",
            "Test Loss: 0.526944, Acc: 0.821656\n",
            "Time:5.2 s\n",
            "80 / 100 Loss: 0.49982741374916045, Acc: 0.8356456756591797\n",
            "Test Loss: 0.526213, Acc: 0.822850\n",
            "Time:5.1 s\n",
            "81 / 100 Loss: 0.49906073353206587, Acc: 0.8356956839561462\n",
            "Test Loss: 0.525580, Acc: 0.822552\n",
            "Time:5.2 s\n",
            "82 / 100 Loss: 0.4983738132512938, Acc: 0.8360125422477722\n",
            "Test Loss: 0.524733, Acc: 0.822552\n",
            "Time:5.2 s\n",
            "83 / 100 Loss: 0.4975826769685542, Acc: 0.8362293243408203\n",
            "Test Loss: 0.523925, Acc: 0.823746\n",
            "Time:5.2 s\n",
            "84 / 100 Loss: 0.4967268899067234, Acc: 0.8363460302352905\n",
            "Test Loss: 0.523266, Acc: 0.823447\n",
            "Time:5.2 s\n",
            "85 / 100 Loss: 0.49590895375040067, Acc: 0.8369130492210388\n",
            "Test Loss: 0.522712, Acc: 0.823248\n",
            "Time:5.3 s\n",
            "86 / 100 Loss: 0.49523874283981933, Acc: 0.8366962671279907\n",
            "Test Loss: 0.521917, Acc: 0.823646\n",
            "Time:5.2 s\n",
            "87 / 100 Loss: 0.49450593280308786, Acc: 0.8368462920188904\n",
            "Test Loss: 0.521522, Acc: 0.823846\n",
            "Time:5.4 s\n",
            "88 / 100 Loss: 0.4938358034115591, Acc: 0.83722984790802\n",
            "Test Loss: 0.521082, Acc: 0.823646\n",
            "Time:5.2 s\n",
            "89 / 100 Loss: 0.49304860865293726, Acc: 0.8372965455055237\n",
            "Test Loss: 0.519965, Acc: 0.823945\n",
            "Time:5.5 s\n",
            "90 / 100 Loss: 0.49221888865198116, Acc: 0.8378134965896606\n",
            "Test Loss: 0.519361, Acc: 0.824542\n",
            "Time:5.2 s\n",
            "91 / 100 Loss: 0.4915709618316007, Acc: 0.8383304476737976\n",
            "Test Loss: 0.518767, Acc: 0.825139\n",
            "Time:5.3 s\n",
            "92 / 100 Loss: 0.4909772543413535, Acc: 0.838280439376831\n",
            "Test Loss: 0.518029, Acc: 0.825040\n",
            "Time:5.2 s\n",
            "93 / 100 Loss: 0.49029802395032845, Acc: 0.8385805487632751\n",
            "Test Loss: 0.517421, Acc: 0.825637\n",
            "Time:5.3 s\n",
            "94 / 100 Loss: 0.48942664866768054, Acc: 0.8385805487632751\n",
            "Test Loss: 0.516865, Acc: 0.825537\n",
            "Time:5.2 s\n",
            "95 / 100 Loss: 0.48894664774009744, Acc: 0.838997483253479\n",
            "Test Loss: 0.516399, Acc: 0.825637\n",
            "Time:5.2 s\n",
            "96 / 100 Loss: 0.4882608980035833, Acc: 0.8387973308563232\n",
            "Test Loss: 0.515845, Acc: 0.825537\n",
            "Time:5.6 s\n",
            "97 / 100 Loss: 0.48776832162506933, Acc: 0.8394810557365417\n",
            "Test Loss: 0.515271, Acc: 0.825338\n",
            "Time:5.4 s\n",
            "98 / 100 Loss: 0.48699696715066376, Acc: 0.8392308950424194\n",
            "Test Loss: 0.514819, Acc: 0.826135\n",
            "Time:5.4 s\n",
            "99 / 100 Loss: 0.48629437173888673, Acc: 0.8394143581390381\n",
            "Test Loss: 0.514120, Acc: 0.826732\n",
            "Time:5.2 s\n",
            "100 / 100 Loss: 0.48573313879292446, Acc: 0.8395811319351196\n",
            "Test Loss: 0.513513, Acc: 0.827130\n",
            "Time:5.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dA3lLGs2v0O",
        "colab_type": "text"
      },
      "source": [
        "## Some tip about axis\n",
        "## https://www.jianshu.com/p/64a7fb9ac310"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwCBmKNsyZbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.tensor([[1,3,7],[4,5,6]])\n",
        "print(torch.max(a,0))\n",
        "print(a.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U80DOx9F8KSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "189e006f-d646-4c8a-f96a-94e8d5b2b180"
      },
      "source": [
        "print(\"{} / {} Loss: {}, Acc: {}\".format(\"1\",\"1\",\"1\",\"1\"))\n",
        "print(\"{} / {} Loss: {}, Acc: {}\".format(100+1,222, 22/2, 222/2))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 / 1 Loss: 1, Acc: 1\n",
            "101 / 222 Loss: 11.0, Acc: 111.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}